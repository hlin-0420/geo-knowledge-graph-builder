"""kg_agent.py
================

An augmented-language-model (LLM) agent that answers questions about Neo4j, the
GEO Help Guide, graphs, Cypher, and generative AI by orchestrating three helper
functions exposed as LangChain `Tool`s.

The public entry-point is :pyfunc:`generate_response`, which routes the user's
question through a ReAct-style agent that decides when (and how) to invoke the
underlying tools.

This module is **import-side-effect-free**: importing it does not trigger any
network calls or heavyweight operations. The LangChain agent and executor are
instantiated eagerly so that a call to :pyfunc:`generate_response` is one RTT.
"""
from __future__ import annotations

from typing import Any, Dict, List

from langchain.agents import AgentExecutor, create_react_agent
from langchain.schema import StrOutputParser
from langchain.tools import Tool
from langchain_core.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

from .llm import llm  # Local LLM wrapper
from ..tools.cypher import run_cypher
from ..tools.vector import find_chunk

###############################################################################
# Prompt & chain definitions                                                   #
###############################################################################

# Template for simple KG chat (no tool‑use reasoning required)
chat_prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(
            "You are a chat bot providing information from the GEO Help Guide. "
            "Answer as usefully and concisely as possible."
        ),
        HumanMessagePromptTemplate.from_template("{input}"),
    ]
)

# Chain: prompt → LLM → string output (via StrOutputParser)
kg_chat = chat_prompt | llm | StrOutputParser()

###############################################################################
# Tool adapter functions                                                      #
###############################################################################

def _general_chat(query: str) -> str:
    """Return a direct answer from the LLM‑only chain.

    The agent calls this tool when **no specialised operation** (vector search
    or Cypher query) is needed.

    Parameters
    ----------
    query:
        The raw user question.

    Returns
    -------
    str
        A concise answer generated by the LLM.
    """

    return kg_chat.invoke({"input": query})


def _lesson_search(q: str) -> str:
    """Retrieve the most relevant lesson chunk for *q* from the vector store.

    Parameters
    ----------
    q:
        Arbitrary free‑text query.

    Returns
    -------
    str
        The retrieved lesson snippet.
    """

    return find_chunk(q)


def _kg_info(cypher: str) -> str:
    """Run a Cypher query against the knowledge graph and return the result.

    Parameters
    ----------
    cypher:
        A **read-only** Cypher statement written by the agent.

    Returns
    -------
    str
        Serialised representation of the query result.
    """

    return run_cypher(cypher)

###############################################################################
# Tool wiring                                                                 #
###############################################################################

# LangChain Tools exposed to the ReAct agent.
# The agent decides *which* tool (if any) to call for a given thought.

tools: List[Tool] = [
    Tool.from_function(
        name="general_chat",
        description="General queries about the knowledge graph",
        func=_general_chat,
    ),
    Tool.from_function(
        name="lesson_content_search",
        description="For retrieving lesson content",
        func=_lesson_search,
    ),
    Tool.from_function(
        name="kg_info",
        description="For retrieving entities and relationships from the knowledge graph",
        func=_kg_info,
    ),
]

###############################################################################
# Agent definition                                                            #
###############################################################################

agent_prompt: PromptTemplate = PromptTemplate.from_template(
    """
You are a Neo4j, Knowledge graph, and generative AI expert.
Be as helpful as possible using only the provided tools.
Only answer questions that relate to Neo4j, graphs, cypher, generative AI, or the GEO Help Guide.

TOOLS:
------

You have access to the following tools:

{tools}

To use a tool, please use the following format:

```
Thought: Do I need to use a tool? Yes
Action: general_chat
Action Input: What types of files are used for curve data?
Observation: Curve data is usually stored in ODF, OIF, or ODT formats in the GEO system.

Action: one of [{tool_names}]
Action Input: <input to the tool>
Observation: <tool result>
```

When you have a response to say to the user, or do not need a tool, use the format:

```
Thought: Do I need to use a tool? No
Final Answer: [your response here]
```

Begin!

New input: {input}
{agent_scratchpad}
"""
)

# Build a ReAct agent and its executor (synchronous‑only for simplicity).
agent = create_react_agent(llm, tools, agent_prompt)
agent_executor: AgentExecutor = AgentExecutor(
    agent=agent,
    tools=tools,
    handle_parsing_errors=False,
    verbose=True,
)

###############################################################################
# Public API                                                                  #
###############################################################################

def generate_response(user_input: str) -> str:
    """Generate a final answer for *user_input* using the configured agent.

    This is the single entry-point consumed by external callers (e.g. a REST
    endpoint or a UI). It hides all LangChain plumbing details.

    Parameters
    ----------
    user_input:
        The user's natural-language question.

    Returns
    -------
    str
        The agent's *Final Answer* (never intermediate scratchpad lines).
    """

    response: Dict[str, Any] = agent_executor.invoke({"input": user_input})
    return response["output"]

__all__: List[str] = [
    "generate_response",
]
